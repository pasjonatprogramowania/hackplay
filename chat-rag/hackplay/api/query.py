import google.generativeai as genai
import numpy as np
from typing import List, Dict

def query(self, question: str, top_k: int = None) -> Dict:
    """Zapytanie RAG"""
    if not self._initialized:
        self.initialize()

    top_k = top_k or self.config.TOP_K_RESULTS

    print(f"üîç Zapytanie: {question}")

    # Embed query - UPROSZCZONE!
    try:
        result = genai.embed_content(
            model=f"models/{self.config.GEMINI_EMBEDDING_MODEL}",
            content=question,
            task_type="retrieval_query"
        )

        # WyciƒÖgnij values bezpo≈õrednio
        print(result)
        query_embedding = np.array([result['embedding']['values']], dtype='float32')

    except Exception as e:
        print(f"‚ùå Query embedding error: {e}")
        import traceback
        traceback.print_exc()  # Print full error
        return {
            "error": str(e),
            "answer": f"B≈ÇƒÖd podczas generowania embedingu: {e}",
            "sources": []
        }

    # Check if index is populated
    if self.index.ntotal == 0:
        return {
            "answer": "Brak dokument√≥w w bazie. Najpierw wrzuƒá pliki przez /api/upload",
            "sources": []
        }

    # Search FAISS
    try:
        distances, indices = self.index.search(query_embedding, top_k)
    except Exception as e:
        print(f"‚ùå FAISS search error: {e}")
        return {
            "error": str(e),
            "answer": f"B≈ÇƒÖd podczas przeszukiwania: {e}",
            "sources": []
        }

    # Retrieve results
    results = []
    context_chunks = []
    for idx, dist in zip(indices[0], distances[0]):
        if idx < len(self.documents) and idx >= 0:
            results.append({
                "chunk": self.documents[idx],
                "metadata": self.metadata[idx],
                "distance": float(dist)
            })
            context_chunks.append(self.documents[idx])

    if len(context_chunks) == 0:
        return {
            "answer": "Nie znaleziono pasujƒÖcych dokument√≥w.",
            "sources": []
        }

    # Build context
    context = "\n\n".join(context_chunks)

    # Generate answer
    prompt = f"""
Jeste≈õ ekspertem prawnym specjalizujƒÖcym siƒô w ustawodawstwie AI. Musisz siƒô jednak zachowywaƒá jak zwyk≈Çy czlowiek.

Wykonuj polecenia zwiƒÖzane z tematami prawnymi, jednak rowniez badz mily witaj sie itp.

Nie hulacynuj. Masz byc pewny co do odpowiedzi.


KONTEKST z dokument√≥w legislacyjnych:
{context}

PYTANIE:
{question}

Odpowied≈∫ powinna byƒá konkretna, oparta na faktach z dokument√≥w i zawieraƒá cytaty z artyku≈Ç√≥w.
"""

    try:
        response = self.model.generate_content(prompt)
        answer_text = response.text
    except Exception as e:
        print(f"‚ùå Error generating answer: {e}")
        answer_text = f"B≈ÇƒÖd generowania odpowiedzi: {e}"

    return {
        "answer": answer_text,
        "sources": results
    }
